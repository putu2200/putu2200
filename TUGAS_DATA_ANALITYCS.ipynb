{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TUGAS DATA ANALITYCS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiRHhfnZdz6YWFSifIOadp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/putu2200/putu2200/blob/main/TUGAS_DATA_ANALITYCS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC3ASkmYkdRQ"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import keras\n",
        "import keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        intersection = (inputs * targets).sum()                            \n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "        \n",
        "        return 1 - dice"
      ],
      "metadata": {
        "id": "VH2Io2Q4lZfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras\n",
        "def DiceLoss(targets, inputs, smooth=1e-6):\n",
        "    \n",
        "    #flatten label and prediction tensors\n",
        "    inputs = K.flatten(inputs)\n",
        "    targets = K.flatten(targets)\n",
        "    \n",
        "    intersection = K.sum(K.dot(targets, inputs))\n",
        "    dice = (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n",
        "    return 1 - dice"
      ],
      "metadata": {
        "id": "EhHrAzX0lkCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        intersection = (inputs * targets).sum()                            \n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "        \n",
        "        return Dice_BCE"
      ],
      "metadata": {
        "id": "9KMODTiYloaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras\n",
        "def DiceBCELoss(targets, inputs, smooth=1e-6):    \n",
        "       \n",
        "    #flatten label and prediction tensors\n",
        "    inputs = K.flatten(inputs)\n",
        "    targets = K.flatten(targets)\n",
        "    \n",
        "    BCE =  binary_crossentropy(targets, inputs)\n",
        "    intersection = K.sum(K.dot(targets, inputs))    \n",
        "    dice_loss = 1 - (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n",
        "    Dice_BCE = BCE + dice_loss\n",
        "    \n",
        "    return Dice_BCE"
      ],
      "metadata": {
        "id": "goql6jzRltlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "class IoULoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(IoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        #intersection is equivalent to True Positive count\n",
        "        #union is the mutually inclusive area of all labels & predictions \n",
        "        intersection = (inputs * targets).sum()\n",
        "        total = (inputs + targets).sum()\n",
        "        union = total - intersection \n",
        "        \n",
        "        IoU = (intersection + smooth)/(union + smooth)\n",
        "                \n",
        "        return 1 - IoU"
      ],
      "metadata": {
        "id": "2JnWqUCilzDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras\n",
        "def IoULoss(targets, inputs, smooth=1e-6):\n",
        "    \n",
        "    #flatten label and prediction tensors\n",
        "    inputs = K.flatten(inputs)\n",
        "    targets = K.flatten(targets)\n",
        "    \n",
        "    intersection = K.sum(K.dot(targets, inputs))\n",
        "    total = K.sum(targets) + K.sum(inputs)\n",
        "    union = total - intersection\n",
        "    \n",
        "    IoU = (intersection + smooth) / (union + smooth)\n",
        "    return 1 - IoU"
      ],
      "metadata": {
        "id": "aU1rVWkzl6UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "ALPHA = 0.8\n",
        "GAMMA = 2\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        #first compute binary cross-entropy \n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        BCE_EXP = torch.exp(-BCE)\n",
        "        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n",
        "                       \n",
        "        return focal_loss"
      ],
      "metadata": {
        "id": "yvgVo4frl99t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras\n",
        "ALPHA = 0.8\n",
        "GAMMA = 2\n",
        "\n",
        "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
        "    \n",
        "    inputs = K.flatten(inputs)\n",
        "    targets = K.flatten(targets)\n",
        "    \n",
        "    BCE = K.binary_crossentropy(targets, inputs)\n",
        "    BCE_EXP = K.exp(-BCE)\n",
        "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
        "    \n",
        "    return focal_loss"
      ],
      "metadata": {
        "id": "TKovaOW7mCuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras\n",
        "ALPHA = 0.8\n",
        "GAMMA = 2\n",
        "\n",
        "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
        "    \n",
        "    inputs = K.flatten(inputs)\n",
        "    targets = K.flatten(targets)\n",
        "    \n",
        "    BCE = K.binary_crossentropy(targets, inputs)\n",
        "    BCE_EXP = K.exp(-BCE)\n",
        "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
        "    \n",
        "    return focal_loss"
      ],
      "metadata": {
        "id": "xy90j_Wcmib5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "ALPHA = 0.5\n",
        "BETA = 0.5\n",
        "\n",
        "class TverskyLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(TverskyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        #True Positives, False Positives & False Negatives\n",
        "        TP = (inputs * targets).sum()    \n",
        "        FP = ((1-targets) * inputs).sum()\n",
        "        FN = (targets * (1-inputs)).sum()\n",
        "       \n",
        "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
        "        \n",
        "        return 1 - Tversky"
      ],
      "metadata": {
        "id": "xShUqCb6mpY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras\n",
        "ALPHA = 0.5\n",
        "BETA = 0.5\n",
        "\n",
        "def TverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, smooth=1e-6):\n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = K.flatten(inputs)\n",
        "        targets = K.flatten(targets)\n",
        "        \n",
        "        #True Positives, False Positives & False Negatives\n",
        "        TP = K.sum((inputs * targets))\n",
        "        FP = K.sum(((1-targets) * inputs))\n",
        "        FN = K.sum((targets * (1-inputs)))\n",
        "       \n",
        "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
        "        \n",
        "        return 1 - Tversky"
      ],
      "metadata": {
        "id": "CWfk5qkrm4CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "ALPHA = 0.5\n",
        "BETA = 0.5\n",
        "GAMMA = 1\n",
        "\n",
        "class FocalTverskyLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(FocalTverskyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        #True Positives, False Positives & False Negatives\n",
        "        TP = (inputs * targets).sum()    \n",
        "        FP = ((1-targets) * inputs).sum()\n",
        "        FN = (targets * (1-inputs)).sum()\n",
        "        \n",
        "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
        "        FocalTversky = (1 - Tversky)**gamma\n",
        "                       \n",
        "        return FocalTversky\n"
      ],
      "metadata": {
        "id": "P-3k3ejDm8ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras\n",
        "ALPHA = 0.5\n",
        "BETA = 0.5\n",
        "GAMMA = 1\n",
        "\n",
        "def FocalTverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, gamma=GAMMA, smooth=1e-6):\n",
        "    \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = K.flatten(inputs)\n",
        "        targets = K.flatten(targets)\n",
        "        \n",
        "        #True Positives, False Positives & False Negatives\n",
        "        TP = K.sum((inputs * targets))\n",
        "        FP = K.sum(((1-targets) * inputs))\n",
        "        FN = K.sum((targets * (1-inputs)))\n",
        "               \n",
        "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
        "        FocalTversky = K.pow((1 - Tversky), gamma)\n",
        "        \n",
        "        return FocalTversky"
      ],
      "metadata": {
        "id": "vzlJjNsenF68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "\n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = scores.view(-1)\n",
        "    labels = labels.view(-1)\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = (labels != ignore)\n",
        "    vscores = scores[valid]\n",
        "    vlabels = labels[valid]\n",
        "    return vscores, vlabels\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    p = len(gt_sorted)\n",
        "    gts = gt_sorted.sum()\n",
        "    intersection = gts - gt_sorted.float().cumsum(0)\n",
        "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
        "    jaccard = 1. - intersection / union\n",
        "    if p > 1: # cover 1-pixel case\n",
        "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
        "    return jaccard\n",
        "\n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                          for log, lab in zip(logits, labels))\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "    if len(labels) == 0:\n",
        "        # only void pixels, the gradients should be 0\n",
        "        return logits.sum() * 0.\n",
        "    signs = 2. * labels.float() - 1.\n",
        "    errors = (1. - logits * Variable(signs))\n",
        "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
        "    perm = perm.data\n",
        "    gt_sorted = labels[perm]\n",
        "    grad = lovasz_grad(gt_sorted)\n",
        "    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
        "    return loss\n",
        "\n",
        "#=====\n",
        "#Multi-class Lovasz loss\n",
        "#=====\n",
        "\n",
        "def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n",
        "              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n",
        "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class labels\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n",
        "                          for prob, lab in zip(probas, labels))\n",
        "    else:\n",
        "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_softmax_flat(probas, labels, classes='present'):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
        "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
        "    \"\"\"\n",
        "    if probas.numel() == 0:\n",
        "        # only void pixels, the gradients should be 0\n",
        "        return probas * 0.\n",
        "    C = probas.size(1)\n",
        "    losses = []\n",
        "    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n",
        "    for c in class_to_sum:\n",
        "        fg = (labels == c).float() # foreground for class c\n",
        "        if (classes is 'present' and fg.sum() == 0):\n",
        "            continue\n",
        "        if C == 1:\n",
        "            if len(classes) > 1:\n",
        "                raise ValueError('Sigmoid output possible only with 1 class')\n",
        "            class_pred = probas[:, 0]\n",
        "        else:\n",
        "            class_pred = probas[:, c]\n",
        "        errors = (Variable(fg) - class_pred).abs()\n",
        "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
        "        perm = perm.data\n",
        "        fg_sorted = fg[perm]\n",
        "        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
        "    return mean(losses)"
      ],
      "metadata": {
        "id": "ZfkzJocjnS_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "class LovaszHingeLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(LovaszHingeLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        inputs = F.sigmoid(inputs)    \n",
        "        Lovasz = lovasz_hinge(inputs, targets, per_image=False)                       \n",
        "        return Lovasz"
      ],
      "metadata": {
        "id": "O4RdIHCrnhVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    gts = tf.reduce_sum(gt_sorted)\n",
        "    intersection = gts - tf.cumsum(gt_sorted)\n",
        "    union = gts + tf.cumsum(1. - gt_sorted)\n",
        "    jaccard = 1. - intersection / union\n",
        "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
        "    return jaccard\n",
        "\n",
        "\n",
        "# --------------------------- BINARY LOSSES ---------------------------\n",
        "\n",
        "\n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        def treat_image(log_lab):\n",
        "            log, lab = log_lab\n",
        "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
        "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
        "            return lovasz_hinge_flat(log, lab)\n",
        "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
        "        loss = tf.reduce_mean(losses)\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss():\n",
        "        labelsf = tf.cast(labels, logits.dtype)\n",
        "        signs = 2. * labelsf - 1.\n",
        "        errors = 1. - logits * tf.stop_gradient(signs)\n",
        "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
        "        gt_sorted = tf.gather(labelsf, perm)\n",
        "        grad = lovasz_grad(gt_sorted)\n",
        "        loss = tf.tensordot(tf.nn.relu(errors_sorted), grad, 1, name=\"loss_non_void\")\n",
        "        return loss\n",
        "\n",
        "    # deal with the void prediction case (only void pixels)\n",
        "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
        "                   lambda: tf.reduce_sum(logits) * 0.,\n",
        "                   compute_loss,\n",
        "                   strict=True,\n",
        "                   name=\"loss\"\n",
        "                   )\n",
        "    return loss\n",
        "\n",
        "\n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = tf.reshape(scores, (-1,))\n",
        "    labels = tf.reshape(labels, (-1,))\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = tf.not_equal(labels, ignore)\n",
        "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
        "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
        "    return vscores, vlabels"
      ],
      "metadata": {
        "id": "SHG90QFGnl1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras\n",
        "# not working yet\n",
        "# def LovaszHingeLoss(inputs, targets):\n",
        "#     return lovasz_hinge_loss(inputs, targets)"
      ],
      "metadata": {
        "id": "DgEanZ-entMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "ALPHA = 0.5 # < 0.5 penalises FP more, > 0.5 penalises FN more\n",
        "CE_RATIO = 0.5 #weighted contribution of modified CE loss compared to Dice loss\n",
        "\n",
        "class ComboLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(ComboLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, eps=1e-9):\n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        #True Positives, False Positives & False Negatives\n",
        "        intersection = (inputs * targets).sum()    \n",
        "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
        "        \n",
        "        inputs = torch.clamp(inputs, eps, 1.0 - eps)       \n",
        "        out = - (ALPHA * ((targets * torch.log(inputs)) + ((1 - ALPHA) * (1.0 - targets) * torch.log(1.0 - inputs))))\n",
        "        weighted_ce = out.mean(-1)\n",
        "        combo = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice)\n",
        "        \n",
        "        return combo"
      ],
      "metadata": {
        "id": "KcqLzHKunwn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras\n",
        "ALPHA = 0.5 # < 0.5 penalises FP more, > 0.5 penalises FN more\n",
        "CE_RATIO = 0.5 #weighted contribution of modified CE loss compared to Dice loss\n",
        "\n",
        "def Combo_loss(targets, inputs, eps=1e-9):\n",
        "    targets = K.flatten(targets)\n",
        "    inputs = K.flatten(inputs)\n",
        "    \n",
        "    intersection = K.sum(targets * inputs)\n",
        "    dice = (2. * intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n",
        "    inputs = K.clip(inputs, eps, 1.0 - eps)\n",
        "    out = - (ALPHA * ((targets * K.log(inputs)) + ((1 - ALPHA) * (1.0 - targets) * K.log(1.0 - inputs))))\n",
        "    weighted_ce = K.mean(out, axis=-1)\n",
        "    combo = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice)\n",
        "    \n",
        "    return combo"
      ],
      "metadata": {
        "id": "Cf1P6f0_n0eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JFBO-jC7n34V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}